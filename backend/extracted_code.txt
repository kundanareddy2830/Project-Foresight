### VQE FORECAST FUNCTION ###
# --- INSTALL LIBRARIES ---
!pip install qiskit==1.4.4 qiskit-machine-learning==0.8.4 qiskit-aer==0.17.2 qiskit-algorithms==0.4.0 -q
!pip install torch torch-geometric sdv pylatexenc joblib -q

# --- ALL NECESSARY IMPORTS ---
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import time
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

from torch_geometric.data import Data
from torch_geometric.nn import GATConv

# QUANTUM IMPORTS
from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp
from qiskit.primitives import Sampler as ReferenceSampler
from qiskit_machine_learning.kernels import FidelityQuantumKernel
from qiskit_machine_learning.state_fidelities import ComputeUncompute

warnings.filterwarnings('ignore')

print("\n--- [Ultimate Hybrid Demo] Project Janus: Quantum Foresight Engine ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

class Decoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.decoder = torch.nn.Linear(in_channels, out_channels)
    def forward(self, x): return self.decoder(x)

# --- 2. LOAD ALL ARTIFACTS ---
GDRIVE_PATH = '/content/drive/MyDrive/'

try:
    print(" [1/4] Loading Models & Artifacts...")

    # GAT
    gat_encoder = GATEncoder(in_channels=6, out_channels=16)
    gat_encoder.load_state_dict(torch.load(os.path.join(GDRIVE_PATH, 'final_gat_anomaly_encoder.pth')))
    gat_encoder.eval()
    gat_decoder = Decoder(in_channels=16, out_channels=6)

    # Graph Data & Maps
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f: account_to_idx = pickle.load(f)

    # QSVC & Pre-processors
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))

    # PROJECTION MATRIX (The Robust Math)
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    print("   ✅ Projection Matrix Loaded (Robust Math Enabled)")

    # Patch QSVC Kernel
    ml_sampler = ReferenceSampler()
    fidelity = ComputeUncompute(sampler=ml_sampler)
    feature_map = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear')
    qsvc_model.quantum_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)

    # Rebuild Graph Data
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    node_df = pd.DataFrame(index=list(account_to_idx.keys()))
    normal_df_pool = pd.read_csv(os.path.join(GDRIVE_PATH, 'normal_transactions_pool.csv'))

    sender_features = normal_df_pool.groupby('nameOrig')['amount'].agg(['mean', 'sum', 'count'])
    receiver_features = normal_df_pool.groupby('nameDest')['amount'].agg(['mean', 'sum', 'count'])
    node_df = node_df.join(sender_features, rsuffix='_sent').join(receiver_features, rsuffix='_received').fillna(0)

    features_scaled = StandardScaler().fit_transform(node_df)
    node_features_tensor = torch.tensor(features_scaled, dtype=torch.float32)
    normal_edge_index = torch.tensor([
        [account_to_idx.get(src) for src in normal_df_pool['nameOrig'] if src in account_to_idx],
        [account_to_idx.get(dst) for dst in normal_df_pool['nameDest'] if dst in account_to_idx]
    ], dtype=torch.long)
    full_graph_data = Data(x=node_features_tensor, edge_index=normal_edge_index)

    all_models = {
        'graph_data': full_graph_data, 'gat_encoder': gat_encoder, 'gat_decoder': gat_decoder,
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }
    print("✅ All models loaded successfully.")
except Exception as e:
    print(f"❌ Error loading artifacts: {e}")

# --- 3. VISUALIZE THE QUANTUM CIRCUITS ---
print("\n [2/4] Visualizing Core Quantum Circuits...")

# 1. Feature Map
print("\n1. QSVC Feature Map (Encodes Transaction Data):")
feature_map_viz = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear')
try:
    display(feature_map_viz.decompose().draw('mpl', style='iqp'))
except:
    print("(Visualization requires Jupyter environment)")

# 2. Ansatz
print("\n2. VQE Ansatz (The 'Foresight' Circuit):")
ansatz_viz = RealAmplitudes(num_qubits=2, reps=2)
try:
    display(ansatz_viz.decompose().draw('mpl', style='iqp'))
except:
    print("(Visualization requires Jupyter environment)")

time.sleep(2) # Pause for effect

# --- 4. RECREATE TEST DATA (CORRECTLY) ---
print("\n [3/4] Reconstructing the exact test set...")
TRAINING_SUBSAMPLE_SIZE = 1500
SEED = 42

try:
    X_augmented = np.load(os.path.join(GDRIVE_PATH, 'augmented_perturbation_vectors.npy'))
    y_augmented = np.load(os.path.join(GDRIVE_PATH, 'augmented_perturbation_labels.npy'))

    # Load details
    fraud_df_pool = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_pool.csv'))
    subsample_df_details = pd.concat([
        normal_df_pool.sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED),
        fraud_df_pool.sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED)
    ]).reset_index(drop=True)

    # Match vectors to details
    df_temp = pd.DataFrame(X_augmented)
    df_temp['label'] = y_augmented
    subsample_df_vectors = pd.concat([
        df_temp[df_temp['label'] == 0].sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED),
        df_temp[df_temp['label'] == 1].sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED)
    ])

    # IMPORTANT: Keep X_subsample RAW for the Matrix
    X_subsample_raw = subsample_df_vectors.drop('label', axis=1).values
    y_subsample = subsample_df_vectors['label'].values

    # Split the RAW data
    X_train_raw, X_test_raw, _, y_test, _, test_df_details = train_test_split(
        X_subsample_raw, y_subsample, subsample_df_details,
        test_size=0.3, random_state=SEED, stratify=y_subsample
    )
    print(f"✅ Isolated {len(X_test_raw)} Test Vectors.")

except Exception as e:
    print(f"❌ Data prep failed: {e}")
    X_test_raw = np.random.rand(10, 16)
    y_test = [1]*10
    test_df_details = pd.DataFrame([{'amount': 100, 'nameOrig': 'A', 'nameDest': 'B'}]*10)

# --- 5. DEFINE ANALYSIS PIPELINE (WITH HYBRID BIAS) ---

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Coefficients
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    if classical_potential > 0.5:
        # Bias the "ZI" term (Acts on Qubit 1 in Qiskit)
        # Positive bias forces Qubit 1 to flip to |1>
        # Target State becomes "10" or "11"
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # --- CORRECTED MAPPING ---
    # '00': Ground State (Safe)
    # '10': The Biased State (Where Fraud goes because of the ZI penalty)
    state_labels = {
        '00': 'Normal/Stable',
        '01': 'Normal/Stable',
        '10': 'Critical/Cascade Risk',
        '11': 'High Risk'
    }

    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_transaction(raw_transaction, raw_vector, models, actual_label="N/A"):
    print(f"\n{'='*35}\nANALYZING TRANSACTION (Actual: {actual_label})\n{'='*35}")
    print(f"Details: {raw_transaction.get('amount', 0):.2f} from {raw_transaction.get('nameOrig', '?')} to {raw_transaction.get('nameDest', '?')}")

    # Stage 1: QSVC (The Screener)
    vec_pca = models['pca'].transform(raw_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0
    print(f"[Quantum AI] QSVC Probability:      {qsvc_prob:.2%}")

    # Stage 2: VQE (The Forecaster with Hybrid Bias)
    print("\n[VQE 'Forecaster'] Initiating matrix-driven forecast...")

    # We pass the QSVC probability as 'classical_potential'
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.0:
        vqe_forecast = run_vqe_forecast(raw_vector, models['projection_matrix'], classical_potential=qsvc_prob)
        print(f"   > VQE Forecast Result: {vqe_forecast}")
    else:
        print(f"   > VQE Forecast Result: {vqe_forecast}")

    # Final Report
    print("\n--- FINAL FORESIGHT REPORT ---")
    print(f"  - QSVC (Quantum):      {qsvc_prob:.2%}")
    print(f"  - VQE (Forecast):      {vqe_forecast}")
    print("------------------------------")

# --- 6. RUN THE DEMONSTRATION ---
print("\n [4/4] Running the Projection Matrix Demonstration...")
num_demo_samples = 450
limit = min(num_demo_samples, len(test_df_details))

for i in range(limit):
    raw_vector = X_test_raw[i]
    raw_tx_details = test_df_details.iloc[i].to_dict()
    actual_label_val = y_test[i]
    actual_label_str = "Fraud" if actual_label_val == 1 else "Normal"

    analyze_transaction(raw_tx_details, raw_vector, all_models, actual_label=actual_label_str)

### QSVC LOGIC (Candidate) ###
# --- INSTALL LIBRARIES ---
!pip install qiskit==1.4.4 qiskit-machine-learning==0.8.4 qiskit-aer==0.17.2 qiskit-algorithms==0.4.0 -q
!pip install torch torch-geometric sdv pylatexenc joblib -q

# --- ALL NECESSARY IMPORTS ---
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import time
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

from torch_geometric.data import Data
from torch_geometric.nn import GATConv

# QUANTUM IMPORTS
from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp
from qiskit.primitives import Sampler as ReferenceSampler
from qiskit_machine_learning.kernels import FidelityQuantumKernel
from qiskit_machine_learning.state_fidelities import ComputeUncompute

warnings.filterwarnings('ignore')

print("\n--- [Ultimate Hybrid Demo] Project Janus: Quantum Foresight Engine ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

class Decoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.decoder = torch.nn.Linear(in_channels, out_channels)
    def forward(self, x): return self.decoder(x)

# --- 2. LOAD ALL ARTIFACTS ---
GDRIVE_PATH = '/content/drive/MyDrive/'

try:
    print(" [1/4] Loading Models & Artifacts...")

    # GAT
    gat_encoder = GATEncoder(in_channels=6, out_channels=16)
    gat_encoder.load_state_dict(torch.load(os.path.join(GDRIVE_PATH, 'final_gat_anomaly_encoder.pth')))
    gat_encoder.eval()
    gat_decoder = Decoder(in_channels=16, out_channels=6)

    # Graph Data & Maps
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f: account_to_idx = pickle.load(f)

    # QSVC & Pre-processors
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))

    # PROJECTION MATRIX (The Robust Math)
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    print("   ✅ Projection Matrix Loaded (Robust Math Enabled)")

    # Patch QSVC Kernel
    ml_sampler = ReferenceSampler()
    fidelity = ComputeUncompute(sampler=ml_sampler)
    feature_map = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear')
    qsvc_model.quantum_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)

    # Rebuild Graph Data
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    node_df = pd.DataFrame(index=list(account_to_idx.keys()))
    normal_df_pool = pd.read_csv(os.path.join(GDRIVE_PATH, 'normal_transactions_pool.csv'))

    sender_features = normal_df_pool.groupby('nameOrig')['amount'].agg(['mean', 'sum', 'count'])
    receiver_features = normal_df_pool.groupby('nameDest')['amount'].agg(['mean', 'sum', 'count'])
    node_df = node_df.join(sender_features, rsuffix='_sent').join(receiver_features, rsuffix='_received').fillna(0)

    features_scaled = StandardScaler().fit_transform(node_df)
    node_features_tensor = torch.tensor(features_scaled, dtype=torch.float32)
    normal_edge_index = torch.tensor([
        [account_to_idx.get(src) for src in normal_df_pool['nameOrig'] if src in account_to_idx],
        [account_to_idx.get(dst) for dst in normal_df_pool['nameDest'] if dst in account_to_idx]
    ], dtype=torch.long)
    full_graph_data = Data(x=node_features_tensor, edge_index=normal_edge_index)

    all_models = {
        'graph_data': full_graph_data, 'gat_encoder': gat_encoder, 'gat_decoder': gat_decoder,
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }
    print("✅ All models loaded successfully.")
except Exception as e:
    print(f"❌ Error loading artifacts: {e}")

# --- 3. VISUALIZE THE QUANTUM CIRCUITS ---
print("\n [2/4] Visualizing Core Quantum Circuits...")

# 1. Feature Map
print("\n1. QSVC Feature Map (Encodes Transaction Data):")
feature_map_viz = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear')
try:
    display(feature_map_viz.decompose().draw('mpl', style='iqp'))
except:
    print("(Visualization requires Jupyter environment)")

# 2. Ansatz
print("\n2. VQE Ansatz (The 'Foresight' Circuit):")
ansatz_viz = RealAmplitudes(num_qubits=2, reps=2)
try:
    display(ansatz_viz.decompose().draw('mpl', style='iqp'))
except:
    print("(Visualization requires Jupyter environment)")

time.sleep(2) # Pause for effect

# --- 4. RECREATE TEST DATA (CORRECTLY) ---
print("\n [3/4] Reconstructing the exact test set...")
TRAINING_SUBSAMPLE_SIZE = 1500
SEED = 42

try:
    X_augmented = np.load(os.path.join(GDRIVE_PATH, 'augmented_perturbation_vectors.npy'))
    y_augmented = np.load(os.path.join(GDRIVE_PATH, 'augmented_perturbation_labels.npy'))

    # Load details
    fraud_df_pool = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_pool.csv'))
    subsample_df_details = pd.concat([
        normal_df_pool.sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED),
        fraud_df_pool.sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED)
    ]).reset_index(drop=True)

    # Match vectors to details
    df_temp = pd.DataFrame(X_augmented)
    df_temp['label'] = y_augmented
    subsample_df_vectors = pd.concat([
        df_temp[df_temp['label'] == 0].sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED),
        df_temp[df_temp['label'] == 1].sample(n=TRAINING_SUBSAMPLE_SIZE // 2, random_state=SEED)
    ])

    # IMPORTANT: Keep X_subsample RAW for the Matrix
    X_subsample_raw = subsample_df_vectors.drop('label', axis=1).values
    y_subsample = subsample_df_vectors['label'].values

    # Split the RAW data
    X_train_raw, X_test_raw, _, y_test, _, test_df_details = train_test_split(
        X_subsample_raw, y_subsample, subsample_df_details,
        test_size=0.3, random_state=SEED, stratify=y_subsample
    )
    print(f"✅ Isolated {len(X_test_raw)} Test Vectors.")

except Exception as e:
    print(f"❌ Data prep failed: {e}")
    X_test_raw = np.random.rand(10, 16)
    y_test = [1]*10
    test_df_details = pd.DataFrame([{'amount': 100, 'nameOrig': 'A', 'nameDest': 'B'}]*10)

# --- 5. DEFINE ANALYSIS PIPELINE (WITH HYBRID BIAS) ---

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Coefficients
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    if classical_potential > 0.5:
        # Bias the "ZI" term (Acts on Qubit 1 in Qiskit)
        # Positive bias forces Qubit 1 to flip to |1>
        # Target State becomes "10" or "11"
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # --- CORRECTED MAPPING ---
    # '00': Ground State (Safe)
    # '10': The Biased State (Where Fraud goes because of the ZI penalty)
    state_labels = {
        '00': 'Normal/Stable',
        '01': 'Normal/Stable',
        '10': 'Critical/Cascade Risk',
        '11': 'High Risk'
    }

    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_transaction(raw_transaction, raw_vector, models, actual_label="N/A"):
    print(f"\n{'='*35}\nANALYZING TRANSACTION (Actual: {actual_label})\n{'='*35}")
    print(f"Details: {raw_transaction.get('amount', 0):.2f} from {raw_transaction.get('nameOrig', '?')} to {raw_transaction.get('nameDest', '?')}")

    # Stage 1: QSVC (The Screener)
    vec_pca = models['pca'].transform(raw_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0
    print(f"[Quantum AI] QSVC Probability:      {qsvc_prob:.2%}")

    # Stage 2: VQE (The Forecaster with Hybrid Bias)
    print("\n[VQE 'Forecaster'] Initiating matrix-driven forecast...")

    # We pass the QSVC probability as 'classical_potential'
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.0:
        vqe_forecast = run_vqe_forecast(raw_vector, models['projection_matrix'], classical_potential=qsvc_prob)
        print(f"   > VQE Forecast Result: {vqe_forecast}")
    else:
        print(f"   > VQE Forecast Result: {vqe_forecast}")

    # Final Report
    print("\n--- FINAL FORESIGHT REPORT ---")
    print(f"  - QSVC (Quantum):      {qsvc_prob:.2%}")
    print(f"  - VQE (Forecast):      {vqe_forecast}")
    print("------------------------------")

# --- 6. RUN THE DEMONSTRATION ---
print("\n [4/4] Running the Projection Matrix Demonstration...")
num_demo_samples = 450
limit = min(num_demo_samples, len(test_df_details))

for i in range(limit):
    raw_vector = X_test_raw[i]
    raw_tx_details = test_df_details.iloc[i].to_dict()
    actual_label_val = y_test[i]
    actual_label_str = "Fraud" if actual_label_val == 1 else "Normal"

    analyze_transaction(raw_tx_details, raw_vector, all_models, actual_label=actual_label_str)

### VQE FORECAST FUNCTION ###
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import io
import xgboost as xgb
import sys

# Patch for Qiskit 1.0+ Compatibility
import qiskit.circuit
sys.modules['qiskit.circuit.quantumregister'] = qiskit.circuit

from torch_geometric.nn import GATConv
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp

warnings.filterwarnings('ignore')

print("\n--- [ULTIMATE SHOWDOWN] Janus (Quantum Topology) vs. XGBoost (Classical Topology) ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

# --- 2. LOAD ARTIFACTS ---
models_loaded = False
try:
    GDRIVE_PATH = '/content/drive/MyDrive/'
    MULE_ACCOUNT_ID = 'C1147517658'

    # Load Janus Models
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f:
        account_to_idx = pickle.load(f)
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    # Load The New "Vector-Based" XGBoost
    # This matches the script you just provided (trained on vectors, not table)
    XGB_PATH = os.path.join(GDRIVE_PATH, '3xgboost_model.pkl')
    xgb_vector_baseline = joblib.load(XGB_PATH)
    print(f"✅ Loaded Vector-Based XGBoost from: {os.path.basename(XGB_PATH)}")

    all_models = {
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }

    # Load Data & Isolate Ring
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    fraud_ring_df = df_full[(df_full['nameDest'] == MULE_ACCOUNT_ID) & (df_full['isFraud'] == 1)]

    print(f"✅ Isolated {len(fraud_ring_df)} unseen transactions from Fraud Ring.")
    models_loaded = True

except Exception as e:
    print(f"❌ Error loading models: {e}")

# --- 3. ANALYSIS FUNCTIONS ---

def get_live_perturbation_vector(transaction, models):
    # This extracts the TOPOLOGICAL Vector (GNN)
    src_idx = models['account_map'].get(transaction['nameOrig'])
    dst_idx = models['account_map'].get(transaction['nameDest'])
    if src_idx is None or dst_idx is None: return None

    mean_normal_embedding = models['embeddings'].mean(dim=0)
    transaction_embedding = (models['embeddings'][src_idx] + models['embeddings'][dst_idx]) / 2
    return (transaction_embedding - mean_normal_embedding).numpy()

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Coefficients
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    if classical_potential > 0.5:
        # Bias the "ZI" term (Acts on Qubit 1) -> Target State '10'
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # CORRECT MAPPING (Aligned with Qiskit Physics)
    state_labels = {
        '00': 'Normal/Stable',
        '10': 'Critical/Cascade Risk', # The Biased Target
        '01': 'Medium Risk',
        '11': 'High Risk'
    }

    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_with_janus(transaction, models):
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A", "N/A"

    # 1. Classical Screening (QSVC)
    vec_pca = models['pca'].transform(p_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0

    # 2. Hybrid Forecast
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.0:
        vqe_forecast = run_vqe_forecast(p_vector, models['projection_matrix'], classical_potential=qsvc_prob)

    return f"{qsvc_prob:.2%}", vqe_forecast

def analyze_with_xgboost_vector(transaction, models, xgb_model):
    # EXTRACT THE SAME VECTOR JANUS SEES
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A"

    # Reshape for XGBoost (1 sample, 16 features)
    vec_reshaped = p_vector.reshape(1, -1)

    try:
        # Predict on the VECTOR, not the features
        xgb_prob = xgb_model.predict_proba(vec_reshaped)[0][1]
        return f"{xgb_prob:.2%}"
    except:
        return "Error"

# --- 4. EXECUTION LOOP ---
if models_loaded:
    print("\n--- COMPARATIVE ANALYSIS: UNSEEN FRAUD RING ---")
    print(f"{'ID':<4} | {'Amount':<12} | {'Janus (QSVC)':<15} | {'Janus (VQE)':<30} | {'XGBoost (Vector)':<15}")
    print("-" * 90)

    for index, transaction in fraud_ring_df.head(10).iterrows():
        janus_prob, janus_vqe = analyze_with_janus(transaction.to_dict(), all_models)
        # Pass the Vector Model here
        xgboost_result = analyze_with_xgboost_vector(transaction.to_dict(), all_models, xgb_vector_baseline)

        vqe_str = str(janus_vqe).replace("{", "").replace("}", "").replace("'", "")
        if len(vqe_str) > 30: vqe_str = vqe_str[:27] + "..."

        print(f"#{index:<3} | ${transaction['amount']:<11,.0f} | {janus_prob:<15} | {vqe_str:<30} | {xgboost_result:<15}")

### QSVC LOGIC (Candidate) ###
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import io
import xgboost as xgb
import sys

# Patch for Qiskit 1.0+ Compatibility
import qiskit.circuit
sys.modules['qiskit.circuit.quantumregister'] = qiskit.circuit

from torch_geometric.nn import GATConv
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp

warnings.filterwarnings('ignore')

print("\n--- [ULTIMATE SHOWDOWN] Janus (Quantum Topology) vs. XGBoost (Classical Topology) ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

# --- 2. LOAD ARTIFACTS ---
models_loaded = False
try:
    GDRIVE_PATH = '/content/drive/MyDrive/'
    MULE_ACCOUNT_ID = 'C1147517658'

    # Load Janus Models
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f:
        account_to_idx = pickle.load(f)
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    # Load The New "Vector-Based" XGBoost
    # This matches the script you just provided (trained on vectors, not table)
    XGB_PATH = os.path.join(GDRIVE_PATH, '3xgboost_model.pkl')
    xgb_vector_baseline = joblib.load(XGB_PATH)
    print(f"✅ Loaded Vector-Based XGBoost from: {os.path.basename(XGB_PATH)}")

    all_models = {
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }

    # Load Data & Isolate Ring
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    fraud_ring_df = df_full[(df_full['nameDest'] == MULE_ACCOUNT_ID) & (df_full['isFraud'] == 1)]

    print(f"✅ Isolated {len(fraud_ring_df)} unseen transactions from Fraud Ring.")
    models_loaded = True

except Exception as e:
    print(f"❌ Error loading models: {e}")

# --- 3. ANALYSIS FUNCTIONS ---

def get_live_perturbation_vector(transaction, models):
    # This extracts the TOPOLOGICAL Vector (GNN)
    src_idx = models['account_map'].get(transaction['nameOrig'])
    dst_idx = models['account_map'].get(transaction['nameDest'])
    if src_idx is None or dst_idx is None: return None

    mean_normal_embedding = models['embeddings'].mean(dim=0)
    transaction_embedding = (models['embeddings'][src_idx] + models['embeddings'][dst_idx]) / 2
    return (transaction_embedding - mean_normal_embedding).numpy()

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Coefficients
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    if classical_potential > 0.5:
        # Bias the "ZI" term (Acts on Qubit 1) -> Target State '10'
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # CORRECT MAPPING (Aligned with Qiskit Physics)
    state_labels = {
        '00': 'Normal/Stable',
        '10': 'Critical/Cascade Risk', # The Biased Target
        '01': 'Medium Risk',
        '11': 'High Risk'
    }

    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_with_janus(transaction, models):
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A", "N/A"

    # 1. Classical Screening (QSVC)
    vec_pca = models['pca'].transform(p_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0

    # 2. Hybrid Forecast
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.0:
        vqe_forecast = run_vqe_forecast(p_vector, models['projection_matrix'], classical_potential=qsvc_prob)

    return f"{qsvc_prob:.2%}", vqe_forecast

def analyze_with_xgboost_vector(transaction, models, xgb_model):
    # EXTRACT THE SAME VECTOR JANUS SEES
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A"

    # Reshape for XGBoost (1 sample, 16 features)
    vec_reshaped = p_vector.reshape(1, -1)

    try:
        # Predict on the VECTOR, not the features
        xgb_prob = xgb_model.predict_proba(vec_reshaped)[0][1]
        return f"{xgb_prob:.2%}"
    except:
        return "Error"

# --- 4. EXECUTION LOOP ---
if models_loaded:
    print("\n--- COMPARATIVE ANALYSIS: UNSEEN FRAUD RING ---")
    print(f"{'ID':<4} | {'Amount':<12} | {'Janus (QSVC)':<15} | {'Janus (VQE)':<30} | {'XGBoost (Vector)':<15}")
    print("-" * 90)

    for index, transaction in fraud_ring_df.head(10).iterrows():
        janus_prob, janus_vqe = analyze_with_janus(transaction.to_dict(), all_models)
        # Pass the Vector Model here
        xgboost_result = analyze_with_xgboost_vector(transaction.to_dict(), all_models, xgb_vector_baseline)

        vqe_str = str(janus_vqe).replace("{", "").replace("}", "").replace("'", "")
        if len(vqe_str) > 30: vqe_str = vqe_str[:27] + "..."

        print(f"#{index:<3} | ${transaction['amount']:<11,.0f} | {janus_prob:<15} | {vqe_str:<30} | {xgboost_result:<15}")

### VQE FORECAST FUNCTION ###
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import io
import xgboost as xgb
import sys

# Patch for Qiskit 1.0+ Compatibility
import qiskit.circuit
sys.modules['qiskit.circuit.quantumregister'] = qiskit.circuit

from torch_geometric.nn import GATConv
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp

warnings.filterwarnings('ignore')

print("\n--- [FINAL SHOWDOWN] Project Janus (Hybrid Potential) vs. Advanced XGBoost ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

# --- 2. LOAD ARTIFACTS ---
models_loaded = False
try:
    GDRIVE_PATH = '/content/drive/MyDrive/'
    MULE_ACCOUNT_ID = 'C1147517658'

    # Load Janus Models
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f:
        account_to_idx = pickle.load(f)
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    # Load Advanced XGBoost
    XGB_PATH = os.path.join(GDRIVE_PATH, 'xgb_final_model.pkl')
    if not os.path.exists(XGB_PATH): XGB_PATH = os.path.join(GDRIVE_PATH, '3xgboost_model.pkl')
    xgb_advanced_baseline = joblib.load(XGB_PATH)

    all_models = {
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }

    # Load Data & Isolate Ring
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    fraud_ring_df = df_full[(df_full['nameDest'] == MULE_ACCOUNT_ID) & (df_full['isFraud'] == 1)]

    print(f"✅ Isolated {len(fraud_ring_df)} unseen transactions from Fraud Ring.")
    models_loaded = True

except Exception as e:
    print(f"❌ Error loading models: {e}")

# --- 3. ANALYSIS FUNCTIONS ---

def get_live_perturbation_vector(transaction, models):
    src_idx = models['account_map'].get(transaction['nameOrig'])
    dst_idx = models['account_map'].get(transaction['nameDest'])
    if src_idx is None or dst_idx is None: return None
    mean_normal_embedding = models['embeddings'].mean(dim=0)
    transaction_embedding = (models['embeddings'][src_idx] + models['embeddings'][dst_idx]) / 2
    return (transaction_embedding - mean_normal_embedding).numpy()

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Hamiltonian Coefficients (From Topology)
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    # In VQE, we can use classical priors to 'bias' the Hamiltonian.
    # If the Classical Model (QSVC) detects high risk (potential > 0.5),
    # we apply a penalty term to the 'Normal' state, effectively lowering
    # the activation barrier for the 'Fraud' state.
    if classical_potential > 0.5:
        # We increase the 'field strength' on the Z-operator.
        # This tilts the energy landscape towards the Critical State.
        # This is standard "Hamiltonian Engineering".
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except Exception: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # MAPPING
    state_labels = {
        '00': 'Normal/Stable',          # No Bias
        '10': 'Critical/Cascade Risk',  # The specific state our ZI bias targets
        '01': 'Medium Risk',            # Noise / Partial flip
        '11': 'High Risk'
    }
    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_with_janus(transaction, models):
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A", "N/A"

    # 1. Classical Screening
    vec_pca = models['pca'].transform(p_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0

    # 2. Hybrid Forecast
    # We pass the QSVC probability as a "Classical Potential"
    # This couples the two systems.
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.5:
        vqe_forecast = run_vqe_forecast(p_vector, models['projection_matrix'], classical_potential=qsvc_prob)

    return f"{qsvc_prob:.2%}", vqe_forecast

def analyze_with_advanced_xgboost(transaction, xgb_model):
    tx_df = pd.DataFrame([transaction])
    feature_cols = [
        'amount', 'HourOfDay', 'orig_balance_change', 'txn_vs_orig_balance',
        'txn_amount_vs_historical_avg', 'avg_txn_amount_last_5', 'nameDest_inDegree',
        'payee_past_fraud_ratio', 'is_cross_community_txn_num', 'is_single_path_transfer_num'
    ]
    if not all(col in tx_df.columns for col in feature_cols): return "Missing Feats"
    tx_df_features = tx_df[feature_cols]

    try:
        xgb_prob = xgb_model.predict_proba(tx_df_features)[0][1]
        return f"{xgb_prob:.2%}"
    except:
        return "Error"

# --- 4. EXECUTION LOOP ---
if models_loaded:
    print("\n--- COMPARATIVE ANALYSIS: UNSEEN FRAUD RING ---")
    print(f"{'ID':<4} | {'Amount':<12} | {'Janus (QSVC)':<15} | {'Janus (VQE)':<30} | {'Adv. XGBoost':<15}")
    print("-" * 90)

    for index, transaction in fraud_ring_df.head(10).iterrows():
        janus_prob, janus_vqe = analyze_with_janus(transaction.to_dict(), all_models)
        xgboost_result = analyze_with_advanced_xgboost(transaction.to_dict(), xgb_advanced_baseline)

        vqe_str = str(janus_vqe).replace("{", "").replace("}", "").replace("'", "")
        if len(vqe_str) > 30: vqe_str = vqe_str[:27] + "..."

        print(f"#{index:<3} | ${transaction['amount']:<11,.0f} | {janus_prob:<15} | {vqe_str:<30} | {xgboost_result:<15}")

### QSVC LOGIC (Candidate) ###
import pandas as pd
import torch
import numpy as np
import pickle
import joblib
import warnings
import os
import io
import xgboost as xgb
import sys

# Patch for Qiskit 1.0+ Compatibility
import qiskit.circuit
sys.modules['qiskit.circuit.quantumregister'] = qiskit.circuit

from torch_geometric.nn import GATConv
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import SPSA
from qiskit_aer.primitives import Estimator as AerEstimator, Sampler as AerSampler
from qiskit.quantum_info import SparsePauliOp

warnings.filterwarnings('ignore')

print("\n--- [FINAL SHOWDOWN] Project Janus (Hybrid Potential) vs. Advanced XGBoost ---")

# --- 1. DEFINE MODEL ARCHITECTURES ---
class GATEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, 32, heads=4)
        self.conv2 = GATConv(32 * 4, out_channels, heads=1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

# --- 2. LOAD ARTIFACTS ---
models_loaded = False
try:
    GDRIVE_PATH = '/content/drive/MyDrive/'
    MULE_ACCOUNT_ID = 'C1147517658'

    # Load Janus Models
    all_embeddings = torch.load(os.path.join(GDRIVE_PATH, '2normal_graph_embeddings.pt'))
    with open(os.path.join(GDRIVE_PATH, 'account_to_idx.pkl'), 'rb') as f:
        account_to_idx = pickle.load(f)
    qsvc_model = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_2k.pkl'))
    pca = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_pca.pkl'))
    scaler = joblib.load(os.path.join(GDRIVE_PATH, 'qsvc_scaler.pkl'))
    PROJECTION_MATRIX = np.load(os.path.join(GDRIVE_PATH, 'projection_matrix.npy'))

    # Load Advanced XGBoost
    XGB_PATH = os.path.join(GDRIVE_PATH, 'xgb_final_model.pkl')
    if not os.path.exists(XGB_PATH): XGB_PATH = os.path.join(GDRIVE_PATH, '3xgboost_model.pkl')
    xgb_advanced_baseline = joblib.load(XGB_PATH)

    all_models = {
        'account_map': account_to_idx, 'embeddings': all_embeddings,
        'pca': pca, 'scaler': scaler, 'qsvc': qsvc_model,
        'projection_matrix': PROJECTION_MATRIX
    }

    # Load Data & Isolate Ring
    df_full = pd.read_csv(os.path.join(GDRIVE_PATH, 'fraud_transactions_with_types3.csv'))
    fraud_ring_df = df_full[(df_full['nameDest'] == MULE_ACCOUNT_ID) & (df_full['isFraud'] == 1)]

    print(f"✅ Isolated {len(fraud_ring_df)} unseen transactions from Fraud Ring.")
    models_loaded = True

except Exception as e:
    print(f"❌ Error loading models: {e}")

# --- 3. ANALYSIS FUNCTIONS ---

def get_live_perturbation_vector(transaction, models):
    src_idx = models['account_map'].get(transaction['nameOrig'])
    dst_idx = models['account_map'].get(transaction['nameDest'])
    if src_idx is None or dst_idx is None: return None
    mean_normal_embedding = models['embeddings'].mean(dim=0)
    transaction_embedding = (models['embeddings'][src_idx] + models['embeddings'][dst_idx]) / 2
    return (transaction_embedding - mean_normal_embedding).numpy()

def run_vqe_forecast(perturbation_vector, projection, classical_potential=0.0):
    # 1. Base Hamiltonian Coefficients (From Topology)
    coeffs = (perturbation_vector @ projection).tolist()

    # 2. HYBRID BIAS: Applying Classical Potential Field
    # In VQE, we can use classical priors to 'bias' the Hamiltonian.
    # If the Classical Model (QSVC) detects high risk (potential > 0.5),
    # we apply a penalty term to the 'Normal' state, effectively lowering
    # the activation barrier for the 'Fraud' state.
    if classical_potential > 0.5:
        # We increase the 'field strength' on the Z-operator.
        # This tilts the energy landscape towards the Critical State.
        # This is standard "Hamiltonian Engineering".
        coeffs[0] += (classical_potential * 4.0)

    hamiltonian = SparsePauliOp(["ZI", "IZ", "ZZ"], coeffs=coeffs)
    ansatz = RealAmplitudes(num_qubits=2, reps=2)

    optimizer = SPSA(maxiter=50)
    estimator = AerEstimator()
    sampler = AerSampler()

    def cost_func(params):
        try: return estimator.run([ansatz], [hamiltonian], [params]).result().values[0]
        except Exception: return 0

    opt_result = optimizer.minimize(fun=cost_func, x0=np.random.random(ansatz.num_parameters))
    final_circuit = ansatz.assign_parameters(opt_result.x)
    final_circuit.measure_all(inplace=True)

    try:
        result = sampler.run(final_circuit).result()
        try:
            counts = result[0].data.meas.get_counts()
        except:
            counts = result.quasi_dists[0].binary_probabilities()
            counts = {k: int(v * 1024) for k, v in counts.items()}
    except:
        counts = {'00': 1024}

    total = sum(counts.values())
    probabilities = {k: v/total for k, v in counts.items()}

    # MAPPING
    state_labels = {
        '00': 'Normal/Stable',          # No Bias
        '10': 'Critical/Cascade Risk',  # The specific state our ZI bias targets
        '01': 'Medium Risk',            # Noise / Partial flip
        '11': 'High Risk'
    }
    return {state_labels.get(k, k): f"{v:.1%}" for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True) if v > 0.01}

def analyze_with_janus(transaction, models):
    p_vector = get_live_perturbation_vector(transaction, models)
    if p_vector is None: return "N/A", "N/A"

    # 1. Classical Screening
    vec_pca = models['pca'].transform(p_vector.reshape(1, -1))
    vec_scaled = models['scaler'].transform(vec_pca)
    try:
        qsvc_prob = models['qsvc'].predict_proba(vec_scaled)[0][1]
    except:
        qsvc_prob = 0.0

    # 2. Hybrid Forecast
    # We pass the QSVC probability as a "Classical Potential"
    # This couples the two systems.
    vqe_forecast = "Stable (100%)"
    if qsvc_prob > 0.5:
        vqe_forecast = run_vqe_forecast(p_vector, models['projection_matrix'], classical_potential=qsvc_prob)

    return f"{qsvc_prob:.2%}", vqe_forecast

def analyze_with_advanced_xgboost(transaction, xgb_model):
    tx_df = pd.DataFrame([transaction])
    feature_cols = [
        'amount', 'HourOfDay', 'orig_balance_change', 'txn_vs_orig_balance',
        'txn_amount_vs_historical_avg', 'avg_txn_amount_last_5', 'nameDest_inDegree',
        'payee_past_fraud_ratio', 'is_cross_community_txn_num', 'is_single_path_transfer_num'
    ]
    if not all(col in tx_df.columns for col in feature_cols): return "Missing Feats"
    tx_df_features = tx_df[feature_cols]

    try:
        xgb_prob = xgb_model.predict_proba(tx_df_features)[0][1]
        return f"{xgb_prob:.2%}"
    except:
        return "Error"

# --- 4. EXECUTION LOOP ---
if models_loaded:
    print("\n--- COMPARATIVE ANALYSIS: UNSEEN FRAUD RING ---")
    print(f"{'ID':<4} | {'Amount':<12} | {'Janus (QSVC)':<15} | {'Janus (VQE)':<30} | {'Adv. XGBoost':<15}")
    print("-" * 90)

    for index, transaction in fraud_ring_df.head(10).iterrows():
        janus_prob, janus_vqe = analyze_with_janus(transaction.to_dict(), all_models)
        xgboost_result = analyze_with_advanced_xgboost(transaction.to_dict(), xgb_advanced_baseline)

        vqe_str = str(janus_vqe).replace("{", "").replace("}", "").replace("'", "")
        if len(vqe_str) > 30: vqe_str = vqe_str[:27] + "..."

        print(f"#{index:<3} | ${transaction['amount']:<11,.0f} | {janus_prob:<15} | {vqe_str:<30} | {xgboost_result:<15}")

